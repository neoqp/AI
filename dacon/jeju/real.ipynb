{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df = df[:10]\n",
    "l = [i for i in range(10)]\n",
    "df2 = pd.DataFrame(l, columns=['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: train.csv | Columns = 7 / 7 | Rows = 59397 -> 59397\n",
      "Loaded data from: test.csv | Columns = 5 / 5 | Rows = 1092 -> 1092\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231102_152220/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231102_152220/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Jan 27 02:56:13 UTC 2023\n",
      "Disk Space Avail:   986.61 GB / 1081.10 GB (91.3%)\n",
      "Train Data Rows:    59397\n",
      "Train Data Columns: 5\n",
      "Label Column: price(원/kg)\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20909.0, 0.0, 1131.68067, 2029.94145)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    243.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.65 MB (7.6% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])                     : 3 | ['item', 'corporation', 'location']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['timestamp']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['item', 'corporation']\n",
      "\t\t('int', ['bool'])            : 1 | ['location']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.2s = Fit runtime\n",
      "\t4 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.55 MB (1.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.0420896678283415, Train Rows: 56897, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-2074.6353\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-2074.6353\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "Warning: Low available memory may cause OOM error if training continues\n",
      "Available Memory: 212 MB\n",
      "Estimated GBM model size: 0 MB\n",
      "Warning: Early stopped GBM model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\t-1934.7766\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "Warning: Low available memory may cause OOM error if training continues\n",
      "Available Memory: 211 MB\n",
      "Estimated GBM model size: 0 MB\n",
      "Warning: Early stopped GBM model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\t-1913.2056\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 0.038 GB out of 0.222 GB available memory (34.174%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=0.48 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\tWarning: Model is expected to require 259.66% of available memory...\n",
      "\tNot enough memory to train RandomForestMSE... Skipping this model.\n",
      "Fitting model: CatBoost ...\n",
      "\t-895.9913\t = Validation score   (-root_mean_squared_error)\n",
      "\t82.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 0.038 GB out of 0.234 GB available memory (32.474%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=0.46 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\tWarning: Model is expected to require 246.89% of available memory...\n",
      "\tNot enough memory to train ExtraTreesMSE... Skipping this model.\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-917.2903\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "Warning: Large XGB model size may cause OOM error if training continues\n",
      "Available Memory: 212 MB\n",
      "Estimated XGB model size: 0 MB\n",
      "\t-2095.0557\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-940.594\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "Warning: Low available memory may cause OOM error if training continues\n",
      "Available Memory: 183 MB\n",
      "Estimated GBM model size: 0 MB\n",
      "Warning: Early stopped GBM model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\t-1949.1041\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-871.052\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 172.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231102_152220/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2391.466064\n",
      "1       -366.456146\n",
      "2       2576.911865\n",
      "3       2671.569580\n",
      "4       2840.131592\n",
      "           ...     \n",
      "1087     881.609985\n",
      "1088     808.764771\n",
      "1089     753.916870\n",
      "1090     698.376831\n",
      "1091     682.604431\n",
      "Name: price(원/kg), Length: 1092, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "\n",
    "train = TabularDataset('train.csv')\n",
    "train = train.drop(['supply(kg)'], axis=1)\n",
    "\n",
    "test = TabularDataset('test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='price(원/kg)').fit(train_data=train)\n",
    "y_pred = predictor.predict(test)\n",
    "print(y_pred)\n",
    "\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df['answer'] = y_pred\n",
    "\n",
    "df.to_csv('submission1.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
